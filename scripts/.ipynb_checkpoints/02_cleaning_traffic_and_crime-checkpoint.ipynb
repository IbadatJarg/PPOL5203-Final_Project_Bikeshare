{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7dbe1c6",
   "metadata": {},
   "source": [
    "<h1><center> Data Cleaning File  <br><br> \n",
    "<font color='grey'> Cleaning Emissions and Traffic Data <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9ffd5",
   "metadata": {},
   "source": [
    "Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7daf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting library\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa853685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traffic_query(coord_list):\n",
    "    \"\"\"\n",
    "    takes in a list of captial bikeshare coordinates\n",
    "    to tell API what data to retrieve\n",
    "    \"\"\"\n",
    "    \n",
    "    #extract coordinates\n",
    "    long = coord_list[0] #UPDATE LATER AFTER SEEING HOW IBADAT SET IT UP\n",
    "    lat = coord_list[1]\n",
    "    \n",
    "    #run query\n",
    "    B_URL = \"https://gis.mwcog.org/wa/rest/services/RTDC/Traffic_Counts_Annual/MapServer/0/query?\" #base url\n",
    "    \n",
    "    response = requests.get(\n",
    "        B_URL,\n",
    "        params = {\n",
    "            \"where\": \"1=1\", #no filters\n",
    "            \"outFields\": \"STATION,COUNTY,AADT2013,AADT2016, AADT2019,XCOORD,YCOORD\", #indicates which cols to return\n",
    "            \"geometry\": f\"{long},{lat}\", #input coordinates\n",
    "            \"geometryType\": \"esriGeometryPoint\", #indicates we're giving it points\n",
    "            \"distance\": 500, #how far away from point\n",
    "            \"units\": \"esriSRUnit_Meter\", #units in meters\n",
    "            \"inSR\": \"4326\", #coordiante system\n",
    "            \"f\": \"json\" #type of file to return\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #check if successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return \"Query Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cb1a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_traffic(geo_json, item_len):\n",
    "    \"\"\"\n",
    "    function to extract columns of interest from traffic jsons\n",
    "    takes in the json created in traffic_query() as well as the length\n",
    "    of response.json()[\"features\"]\n",
    "    \"\"\"\n",
    "    #storing\n",
    "    temp = []\n",
    "    \n",
    "    for item in range(item_len):\n",
    "        obs = {\n",
    "                \"id\": geo_json[\"features\"][item][\"attributes\"][\"STATION\"],\n",
    "                \"x\": geo_json[\"features\"][item][\"attributes\"][\"XCOORD\"],\n",
    "                \"y\": geo_json[\"features\"][item][\"attributes\"][\"YCOORD\"],\n",
    "                \"county\": geo_json[\"features\"][item][\"attributes\"][\"COUNTY\"],\n",
    "                \"2013\": geo_json[\"features\"][item][\"attributes\"][\"AADT2013\"],\n",
    "                \"2016\": geo_json[\"features\"][item][\"attributes\"][\"AADT2016\"],\n",
    "                \"2019\": geo_json[\"features\"][item][\"attributes\"][\"AADT2019\"]\n",
    "            }\n",
    "        temp.append(obs)    \n",
    "        \n",
    "    #return \n",
    "    return(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f53da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read open capital bikeshare file\n",
    "coord_df = pd.read_csv(\"../data/raw_data/opened_capital_bikes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52d4a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for needed columns\n",
    "coord_df = coord_df.filter([\"STATION_ID\", \"LATITUDE\", \"LONGITUDE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da4f2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "coord_df = coord_df.rename(columns = {\"STATION_ID\": \"cb_station\", \"LATITUDE\": \"lat\", \"LONGITUDE\": \"long\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "813a5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "traffic_df = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dacb2a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_27544\\513894237.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n",
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_27544\\513894237.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "#script to get average traffic volume for all opened capital bike shares\n",
    "for xy in range(len(coord_df)):\n",
    "    #runs query for coordinates\n",
    "    test_json = traffic_query([coord_df[\"long\"][xy], coord_df[\"lat\"][xy]])\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test_json == \"Query Failed\" or len(test_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        #append it to traffic_df\n",
    "        traffic_df = pd.concat([traffic_df, append], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test_clean = clean_traffic(test_json, len(test_json[\"features\"]))\n",
    "    #converts to pandas and filters for where there is data for both 2013 and 2019\n",
    "    test_df = pd.DataFrame(test_clean)\n",
    "    test_df = test_df[(~test_df['2013'].isna()) & (~test_df['2019'].isna())]\n",
    "    \n",
    "    #adds if-else statement \n",
    "    if len(test_df) == 0: #in case there is no row with data for both\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        #append it to traffic_df\n",
    "        traffic_df = pd.concat([traffic_df, append], ignore_index = True)\n",
    "        continue\n",
    "    elif len(test_df) == 1: #if there is only one row and we can't compute SEM\n",
    "        test_df = (test_df.assign(change_AADT = test_df[\"2013\"] - test_df[\"2019\"]).\n",
    "         filter([\"change_AADT\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_AADT').\n",
    "         rename(columns = {\"mean\" : \"change_AADT_mean\",\n",
    "                           \"sem\" : \"change_AADT_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "        test_df['change_AADT_sem'] = np.nan\n",
    "    else:\n",
    "        #enough rows to compute both mean change in traffic volume & standard error\n",
    "        test_df = (test_df.assign(change_AADT = test_df[\"2013\"] - test_df[\"2019\"]).\n",
    "         filter([\"change_AADT\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_AADT').\n",
    "         rename(columns = {\"mean\" : \"change_AADT_mean\",\n",
    "                           \"sem\" : \"change_AADT_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "    #adds station and search coordinates\n",
    "    test_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "    test_df['long'] = coord_df[\"long\"][xy]\n",
    "    test_df['lat'] = coord_df[\"lat\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    test_df = test_df[['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat']]\n",
    "    #add to storage dataframe\n",
    "    traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d084414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saving traffic data for opened capital bikes\n",
    "#traffic_df.to_csv(\"opened_cb_traffic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48c482d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading unopened capital bikeshare files (proposed bikeshares)\n",
    "unopened_df = pd.read_excel(\"../data/raw_data/unopened_capital_bikes_proposed.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d75bbf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for needed columns\n",
    "unopened_df = unopened_df.filter([\"FID\", \"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e28f864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "unopened_df = unopened_df.rename(columns = {\"FID\": \"cb_station\", \"y\": \"lat\", \"x\": \"long\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8d61075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "unopened_traffic_df = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2b67fe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_27544\\727394444.py:70: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unopened_traffic_df = pd.concat([unopened_traffic_df, test_df], ignore_index = True)\n",
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_27544\\727394444.py:70: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unopened_traffic_df = pd.concat([unopened_traffic_df, test_df], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "#script to get average traffic volume for all unopened capital bike shares\n",
    "for xy in range(len(unopened_df)):\n",
    "    #runs query for coordinates\n",
    "    test_json = traffic_query([unopened_df[\"long\"][xy], unopened_df[\"lat\"][xy]])\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test_json == \"Query Failed\" or len(test_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = unopened_df[\"cb_station\"][xy]\n",
    "        append['long'] = unopened_df[\"long\"][xy]\n",
    "        append['lat'] = unopened_df[\"lat\"][xy]\n",
    "        #append it to traffic_df\n",
    "        unopened_traffic_df = pd.concat([unopened_traffic_df, append], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test_clean = clean_traffic(test_json, len(test_json[\"features\"]))\n",
    "    #converts to pandas and filters for where there is data for both 2013 and 2019\n",
    "    test_df = pd.DataFrame(test_clean)\n",
    "    test_df = test_df[(~test_df['2013'].isna()) & (~test_df['2019'].isna())]\n",
    "    \n",
    "    #adds if-else statement \n",
    "    if len(test_df) == 0: #in case there is no row with data for both\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = unopened_df[\"cb_station\"][xy]\n",
    "        append['long'] = unopened_df[\"long\"][xy]\n",
    "        append['lat'] = unopened_df[\"lat\"][xy]\n",
    "        #append it to traffic_df\n",
    "        traffic_df = pd.concat([unopened_traffic_df, append], ignore_index = True)\n",
    "        continue\n",
    "    elif len(test_df) == 1: #if there is only one row and we can't compute SEM\n",
    "        test_df = (test_df.assign(change_AADT = test_df[\"2013\"] - test_df[\"2019\"]).\n",
    "         filter([\"change_AADT\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_AADT').\n",
    "         rename(columns = {\"mean\" : \"change_AADT_mean\",\n",
    "                           \"sem\" : \"change_AADT_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "        test_df['change_AADT_sem'] = np.nan\n",
    "    else:\n",
    "        #enough rows to compute both mean change in traffic volume & standard error\n",
    "        test_df = (test_df.assign(change_AADT = test_df[\"2013\"] - test_df[\"2019\"]).\n",
    "         filter([\"change_AADT\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_AADT').\n",
    "         rename(columns = {\"mean\" : \"change_AADT_mean\",\n",
    "                           \"sem\" : \"change_AADT_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "    #adds station and search coordinates\n",
    "    test_df['station'] = unopened_df[\"cb_station\"][xy]\n",
    "    test_df['long'] = unopened_df[\"long\"][xy]\n",
    "    test_df['lat'] = unopened_df[\"lat\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    test_df = test_df[['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat']]\n",
    "    #add to storage dataframe\n",
    "    unopened_traffic_df = pd.concat([unopened_traffic_df, test_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e4d21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving traffic data for unopened capital bikes\n",
    "#unopened_traffic_df.to_csv(\"unopened_cb_traffic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec22ff1",
   "metadata": {},
   "source": [
    "Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9754abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crime_query(coord_list, year):\n",
    "    \"\"\"\n",
    "    takes in a list of captial bikeshare coordinates\n",
    "    to tell API what data to retrieve\n",
    "    and the year to indicate which API to utilize\n",
    "    \"\"\"\n",
    "    \n",
    "    #extract coordinates\n",
    "    long = coord_list[0] #UPDATE LATER AFTER SEEING HOW IBADAT SET IT UP\n",
    "    lat = coord_list[1]\n",
    "    \n",
    "    #run query\n",
    "    if year == 2019:\n",
    "        B_URL = \"https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/10/query?\" #base url for 2019\n",
    "    elif year == 2013:\n",
    "        B_URL = \"https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/1/query?\" #base url for 2013\n",
    "    \n",
    "    response = requests.get(\n",
    "        B_URL,\n",
    "        params = {\n",
    "            \"where\": \"1=1\", #no filters\n",
    "            \"outFields\": \"CCN,OFFENSE,WARD,METHOD,SHIFT,LONGITUDE,LATITUDE\", #indicates which cols to return\n",
    "            \"geometry\": f\"{long},{lat}\", #input coordinates\n",
    "            \"geometryType\": \"esriGeometryPoint\", #indicates we're giving it points\n",
    "            \"distance\": 500, #how far away from point\n",
    "            \"units\": \"esriSRUnit_Meter\", #units in meters\n",
    "            \"inSR\": \"4326\", #coordiante system\n",
    "            \"f\": \"json\" #type of file to return\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #check if successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return \"Query Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "082c5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_crime(geo_json, item_len, year):\n",
    "    \"\"\"\n",
    "    function to extract columns of interest from crime jsons\n",
    "    takes in the json created in crime_query() as well as the length\n",
    "    of response.json()[\"features\"]. also a int indicating year of crime api\n",
    "    \"\"\"\n",
    "    #storing\n",
    "    temp = []\n",
    "    \n",
    "    for item in range(item_len):\n",
    "        obs = {\n",
    "                \"id\": geo_json[\"features\"][item][\"attributes\"][\"CCN\"],\n",
    "                \"x\": geo_json[\"features\"][item][\"attributes\"][\"LONGITUDE\"],\n",
    "                \"y\": geo_json[\"features\"][item][\"attributes\"][\"LATITUDE\"],\n",
    "                \"ward\": geo_json[\"features\"][item][\"attributes\"][\"WARD\"],\n",
    "                \"method\": geo_json[\"features\"][item][\"attributes\"][\"METHOD\"],\n",
    "                \"shift\": geo_json[\"features\"][item][\"attributes\"][\"SHIFT\"],\n",
    "                \"offense\": geo_json[\"features\"][item][\"attributes\"][\"OFFENSE\"],\n",
    "                \"year\": year\n",
    "            }\n",
    "        temp.append(obs)    \n",
    "        \n",
    "    #return \n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b31a7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "crime_df = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5ef6197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_27544\\3286114899.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  crime_df = pd.concat([crime_df, append], ignore_index = True)\n",
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_27544\\3286114899.py:88: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  crime_df = pd.concat([crime_df, merged_df], ignore_index = True)\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(coord_df)):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#runs query for coordinates\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     test13_json \u001b[38;5;241m=\u001b[39m crime_query([coord_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m][xy], coord_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m][xy]], \u001b[38;5;241m2013\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     test19_json \u001b[38;5;241m=\u001b[39m \u001b[43mcrime_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcoord_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlong\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2019\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#adds if statement in case query fails or returns no coordinates\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test13_json \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery Failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m test19_json \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery Failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test13_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test19_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m#creates dataframe with NaN values for AADT\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m, in \u001b[0;36mcrime_query\u001b[1;34m(coord_list, year)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m year \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2013\u001b[39m:\n\u001b[0;32m     16\u001b[0m     B_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/1/query?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#base url for 2013\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mB_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhere\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1=1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#no filters\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutFields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCCN,OFFENSE,WARD,METHOD,SHIFT,LONGITUDE,LATITUDE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#indicates which cols to return\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeometry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlong\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlat\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#input coordinates\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeometryType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mesriGeometryPoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#indicates we're giving it points\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#how far away from point\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mesriSRUnit_Meter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#units in meters\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minSR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4326\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#coordiante system\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#type of file to return\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#check if successful\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "#script to get average crime for all opened capital bike shares\n",
    "for xy in range(len(coord_df)):\n",
    "    #runs query for coordinates\n",
    "    test13_json = crime_query([coord_df[\"long\"][xy], coord_df[\"lat\"][xy]], 2013)\n",
    "    test19_json = crime_query([coord_df[\"long\"][xy], coord_df[\"lat\"][xy]], 2019)\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test13_json == \"Query Failed\" or test19_json == \"Query Failed\" or len(test13_json[\"features\"]) == 0 or len(test19_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        #append it to crime_df\n",
    "        crime_df = pd.concat([crime_df, append], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test13_clean = clean_crime(test13_json, len(test13_json[\"features\"]), 2013)\n",
    "    test19_clean = clean_crime(test19_json, len(test19_json[\"features\"]), 2019)\n",
    "    \n",
    "    #converts to pandas\n",
    "    test13_df = pd.DataFrame(test13_clean)\n",
    "    test19_df = pd.DataFrame(test19_clean)\n",
    "    \n",
    "    #compute crime count\n",
    "    test13_df = test13_df.assign(crime_sum = len(test13_df)).filter(['crime_sum']).drop_duplicates()\n",
    "    test13_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "    test19_df = test19_df.assign(crime_sum = len(test19_df)).filter(['crime_sum']).drop_duplicates()\n",
    "    test19_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "\n",
    "    #rename cols\n",
    "    test13_df = test13_df.rename(columns={col: col + '_13' for col in test13_df.columns if col != 'station'})\n",
    "    test19_df = test19_df.rename(columns={col: col + '_19' for col in test19_df.columns if col != 'station'})\n",
    "    \n",
    "    #merge df\n",
    "    merged_df = pd.merge(test13_df, test19_df, on='station', how='outer')\n",
    "    \n",
    "    #filters for where there is data for both 2013 and 2019\n",
    "    merged_df = merged_df[(~merged_df['crime_sum_13'].isna()) & (~merged_df['crime_sum_19'].isna())]\n",
    "    \n",
    "    #adds if-else statement \n",
    "    if len(merged_df) == 0: #in case there is no row with data for both\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        #append it to crime_df\n",
    "        crime_df = pd.concat([crime_df, append], ignore_index = True)\n",
    "        continue\n",
    "    elif len(merged_df) == 1: #if there is only one row and we can't compute SEM\n",
    "        merged_df = (merged_df.assign(change_crime = merged_df[\"crime_sum_13\"] - merged_df[\"crime_sum_19\"]).\n",
    "         filter([\"change_crime\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_crime').\n",
    "         rename(columns = {\"mean\" : \"change_CRIME_mean\",\n",
    "                           \"sem\" : \"change_CRIME_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "        merged_df['change_CRIME_sem'] = np.nan\n",
    "    else:\n",
    "        #enough rows to compute both mean change in crime volume & standard error\n",
    "        merged_df = (merged_df.assign(change_crime = merged_df[\"crime_sum_13\"] - merged_df[\"crime_sum_19\"]).\n",
    "         filter([\"change_crime\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_crime').\n",
    "         rename(columns = {\"mean\" : \"change_CRIME_mean\",\n",
    "                           \"sem\" : \"change_CRIME_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "    #adds station and search coordinates\n",
    "    merged_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "    merged_df['long'] = coord_df[\"long\"][xy]\n",
    "    merged_df['lat'] = coord_df[\"lat\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    merged_df = merged_df[['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat']]\n",
    "    #add to storage dataframe\n",
    "    crime_df = pd.concat([crime_df, merged_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec7bd78b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>change_CRIME_mean</th>\n",
       "      <th>change_CRIME_sem</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83fa4699-b2ee-4f14-af3b-167f34a62fb4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.221807</td>\n",
       "      <td>38.904415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e07eed7c-2d2a-4f13-aff7-2be6500958f7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.095513</td>\n",
       "      <td>38.916561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08249ef2-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>205.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.041779</td>\n",
       "      <td>38.905067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>082544b7-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.077078</td>\n",
       "      <td>38.943837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>082524a2-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.086063</td>\n",
       "      <td>38.893237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>c778c501-ba47-479b-b652-831432ffd74f</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.005497</td>\n",
       "      <td>38.824481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>08249cd3-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.046615</td>\n",
       "      <td>38.896114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0825b983-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.015360</td>\n",
       "      <td>38.907333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>08251bba-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.107735</td>\n",
       "      <td>38.876393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>08254770-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>-263.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.041571</td>\n",
       "      <td>38.918809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  station  change_CRIME_mean change_CRIME_sem  \\\n",
       "0    83fa4699-b2ee-4f14-af3b-167f34a62fb4                NaN              NaN   \n",
       "1    e07eed7c-2d2a-4f13-aff7-2be6500958f7                4.0              NaN   \n",
       "2    08249ef2-1f3f-11e7-bf6b-3863bb334450              205.0              NaN   \n",
       "3    082544b7-1f3f-11e7-bf6b-3863bb334450               -2.0              NaN   \n",
       "4    082524a2-1f3f-11e7-bf6b-3863bb334450                NaN              NaN   \n",
       "..                                    ...                ...              ...   \n",
       "388  c778c501-ba47-479b-b652-831432ffd74f              -75.0              NaN   \n",
       "389  08249cd3-1f3f-11e7-bf6b-3863bb334450               -7.0              NaN   \n",
       "390  0825b983-1f3f-11e7-bf6b-3863bb334450              -11.0              NaN   \n",
       "391  08251bba-1f3f-11e7-bf6b-3863bb334450                NaN              NaN   \n",
       "392  08254770-1f3f-11e7-bf6b-3863bb334450             -263.0              NaN   \n",
       "\n",
       "          long        lat  \n",
       "0   -77.221807  38.904415  \n",
       "1   -77.095513  38.916561  \n",
       "2   -77.041779  38.905067  \n",
       "3   -77.077078  38.943837  \n",
       "4   -77.086063  38.893237  \n",
       "..         ...        ...  \n",
       "388 -77.005497  38.824481  \n",
       "389 -77.046615  38.896114  \n",
       "390 -77.015360  38.907333  \n",
       "391 -77.107735  38.876393  \n",
       "392 -77.041571  38.918809  \n",
       "\n",
       "[393 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving crime data for opened capital bikes\n",
    "#crime_df.to_csv(\"opened_cb_crime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e31e04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "unopened_crime_df = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d15a542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_27544\\2137593135.py:89: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unopened_crime_df = pd.concat([unopened_crime_df, merged_df], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "#script to get average crime for all unopened capital bike shares\n",
    "for xy in range(len(unopened_df)):\n",
    "    #runs query for coordinates\n",
    "    test13_json = crime_query([unopened_df[\"long\"][xy], unopened_df[\"lat\"][xy]], 2013)\n",
    "    test19_json = crime_query([unopened_df[\"long\"][xy], unopened_df[\"lat\"][xy]], 2019)\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test13_json == \"Query Failed\" or test19_json == \"Query Failed\" or len(test13_json[\"features\"]) == 0 or len(test19_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = unopened_df[\"cb_station\"][xy]\n",
    "        append['long'] = unopened_df[\"long\"][xy]\n",
    "        append['lat'] = unopened_df[\"lat\"][xy]\n",
    "        #append it to crime_df\n",
    "        unopened_crime_df = pd.concat([unopened_crime_df, append], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test13_clean = clean_crime(test13_json, len(test13_json[\"features\"]), 2013)\n",
    "    test19_clean = clean_crime(test19_json, len(test19_json[\"features\"]), 2019)\n",
    "    \n",
    "    #converts to pandas\n",
    "    test13_df = pd.DataFrame(test13_clean)\n",
    "    test19_df = pd.DataFrame(test19_clean)\n",
    "    \n",
    "    #compute crime count\n",
    "    test13_df = test13_df.assign(crime_sum = len(test13_df)).filter(['crime_sum']).drop_duplicates()\n",
    "    test13_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "    test19_df = test19_df.assign(crime_sum = len(test19_df)).filter(['crime_sum']).drop_duplicates()\n",
    "    test19_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "\n",
    "    #rename cols\n",
    "    test13_df = test13_df.rename(columns={col: col + '_13' for col in test13_df.columns if col != 'station'})\n",
    "    test19_df = test19_df.rename(columns={col: col + '_19' for col in test19_df.columns if col != 'station'})\n",
    "    \n",
    "    #merge df\n",
    "    merged_df = pd.merge(test13_df, test19_df, on='station', how='outer')\n",
    "    \n",
    "    #filters for where there is data for both 2013 and 2019\n",
    "    merged_df = merged_df[(~merged_df['crime_sum_13'].isna()) & (~merged_df['crime_sum_19'].isna())]\n",
    "    \n",
    "    #adds if-else statement \n",
    "    if len(merged_df) == 0: #in case there is no row with data for both\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = unopened_df[\"cb_station\"][xy]\n",
    "        append['long'] = unopened_df[\"long\"][xy]\n",
    "        append['lat'] = unopened_df[\"lat\"][xy]\n",
    "        #append it to crime_df\n",
    "        unopened_crime_df = pd.concat([unopened_crime_df, append], ignore_index = True)\n",
    "        continue\n",
    "    elif len(merged_df) == 1: #if there is only one row and we can't compute SEM\n",
    "        merged_df = (merged_df.assign(change_crime = merged_df[\"crime_sum_13\"] - merged_df[\"crime_sum_19\"]).\n",
    "         filter([\"change_crime\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_crime').\n",
    "         rename(columns = {\"mean\" : \"change_CRIME_mean\",\n",
    "                           \"sem\" : \"change_CRIME_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "        merged_df['change_CRIME_sem'] = np.nan\n",
    "    else:\n",
    "        #enough rows to compute both mean change in crime volume & standard error\n",
    "        merged_df = (merged_df.assign(change_crime = merged_df[\"crime_sum_13\"] - merged_df[\"crime_sum_19\"]).\n",
    "         filter([\"change_crime\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_crime').\n",
    "         rename(columns = {\"mean\" : \"change_CRIME_mean\",\n",
    "                           \"sem\" : \"change_CRIME_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "    #adds station and search coordinates\n",
    "    merged_df['station'] = unopened_df[\"cb_station\"][xy]\n",
    "    merged_df['long'] = unopened_df[\"long\"][xy]\n",
    "    merged_df['lat'] = unopened_df[\"lat\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    merged_df = merged_df[['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat']]\n",
    "    #add to storage dataframe\n",
    "    unopened_crime_df = pd.concat([unopened_crime_df, merged_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46096057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving crime data for unopened capital bikes\n",
    "#unopened_crime_df.to_csv(\"unopened_cb_crime.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

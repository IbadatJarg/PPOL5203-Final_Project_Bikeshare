{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7dbe1c6",
   "metadata": {},
   "source": [
    "<h1><center> Data Cleaning File  <br><br> \n",
    "<font color='grey'> Cleaning Emissions and Traffic Data <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9ffd5",
   "metadata": {},
   "source": [
    "Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7daf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting library\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa853685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traffic_query(coord_list):\n",
    "    \"\"\"\n",
    "    takes in a list of captial bikeshare coordinates\n",
    "    to tell API what data to retrieve\n",
    "    \"\"\"\n",
    "    \n",
    "    #extract coordinates\n",
    "    long = coord_list[0] #UPDATE LATER AFTER SEEING HOW IBADAT SET IT UP\n",
    "    lat = coord_list[1]\n",
    "    \n",
    "    #run query\n",
    "    B_URL = \"https://gis.mwcog.org/wa/rest/services/RTDC/Traffic_Counts_Annual/MapServer/0/query?\" #base url\n",
    "    \n",
    "    response = requests.get(\n",
    "        B_URL,\n",
    "        params = {\n",
    "            \"where\": \"1=1\", #no filters\n",
    "            \"outFields\": \"AADT2010,AADT2011,AADT2012,AADT2013,AADT2014,AADT2015,AADT2016\", #indicates which cols to return\n",
    "            \"geometry\": f\"{long},{lat}\", #input coordinates\n",
    "            \"geometryType\": \"esriGeometryPoint\", #indicates we're giving it points\n",
    "            \"distance\": 500, #how far away from point\n",
    "            \"units\": \"esriSRUnit_Meter\", #units in meters\n",
    "            \"inSR\": \"4326\", #coordiante system\n",
    "            \"f\": \"json\" #type of file to return\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #check if successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return \"Query Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cb1a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_traffic(geo_json, item_len):\n",
    "    \"\"\"\n",
    "    function to extract columns of interest from traffic jsons\n",
    "    takes in the json created in traffic_query() as well as the length\n",
    "    of response.json()[\"features\"]\n",
    "    \"\"\"\n",
    "    #storing\n",
    "    temp = []\n",
    "    \n",
    "    #grabbing cols\n",
    "    for item in range(item_len):\n",
    "        obs = {\n",
    "                \"2010\": geo_json[\"features\"][item][\"attributes\"][\"AADT2010\"],\n",
    "                \"2011\": geo_json[\"features\"][item][\"attributes\"][\"AADT2011\"],\n",
    "                \"2012\": geo_json[\"features\"][item][\"attributes\"][\"AADT2012\"],\n",
    "                \"2013\": geo_json[\"features\"][item][\"attributes\"][\"AADT2013\"],\n",
    "                \"2014\": geo_json[\"features\"][item][\"attributes\"][\"AADT2014\"],\n",
    "                \"2015\": geo_json[\"features\"][item][\"attributes\"][\"AADT2015\"],\n",
    "                \"2016\": geo_json[\"features\"][item][\"attributes\"][\"AADT2016\"]\n",
    "            }\n",
    "        #appending to list\n",
    "        temp.append(obs)    \n",
    "        \n",
    "    #return \n",
    "    return(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f53da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read open capital bikeshare file\n",
    "coord_df = pd.read_csv(\"../data/raw_data/opened_capital_bikes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52d4a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for needed columns\n",
    "coord_df = coord_df.filter([\"STATION_ID\", \"LATITUDE\", \"LONGITUDE\", \"Opening Year\", \"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "da4f2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "coord_df = coord_df.rename(columns = {\"STATION_ID\": \"cb_station\", \"LATITUDE\": \"lat\", \"LONGITUDE\": \"long\", \"Opening Year\": \"open_year\", \"Name\": \"name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "813a5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "traffic_df = pd.DataFrame(columns = ['id', 'long', 'lat', 'open_year', 'name','2010', '2011', '2012', '2013', '2014', '2015'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92286b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_clean)\n",
    "test_df =  (test_df.agg([\"mean\"]).\n",
    "         reset_index(drop = True)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dacb2a28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_30636\\2884552928.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n",
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_30636\\2884552928.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "#script to get average traffic volume for all opened capital bike shares\n",
    "for xy in range(len(coord_df)):\n",
    "    #runs query for coordinates\n",
    "    test_json = traffic_query([coord_df[\"long\"][xy], coord_df[\"lat\"][xy]])\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test_json == \"Query Failed\" or len(test_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns = ['id', 'long', 'lat', 'open_year', 'name','2010', '2011', '2012', '2013', '2014', '2015'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['id'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        append['open_year'] = coord_df[\"open_year\"][xy]\n",
    "        append['name'] = coord_df[\"name\"][xy]\n",
    "        #append it to traffic_df\n",
    "        traffic_df = pd.concat([traffic_df, append], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test_clean = clean_traffic(test_json, len(test_json[\"features\"]))\n",
    "    #converts to pandas and filters for where there is data for both 2013 and 2019\n",
    "    test_df = pd.DataFrame(test_clean)\n",
    "\n",
    "    #gets mean traffic volume for all years\n",
    "    test_df = test_df.agg([\"mean\"]).reset_index(drop = True)\n",
    "\n",
    "    #adds station and search coordinates\n",
    "    test_df['id'] = coord_df[\"cb_station\"][xy]\n",
    "    test_df['long'] = coord_df[\"long\"][xy]\n",
    "    test_df['lat'] = coord_df[\"lat\"][xy]\n",
    "    test_df['open_year'] = coord_df[\"open_year\"][xy]\n",
    "    test_df['name'] = coord_df[\"name\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    test_df = test_df[['id', 'long', 'lat', 'open_year', 'name','2010', '2011', '2012', '2013', '2014', '2015', '2016']]\n",
    "    #add to storage dataframe\n",
    "    traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d084414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saving traffic data for opened capital bikes\n",
    "traffic_df.to_csv(\"opened_cb_traffic.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "48c482d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading unopened capital bikeshare files (proposed bikeshares)\n",
    "unopened_df = pd.read_excel(\"../data/raw_data/unopened_capital_bikes_proposed.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d75bbf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for needed columns\n",
    "unopened_df = unopened_df.filter([\"FID\", \"x\", \"y\", \"ClosestInt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e28f864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "unopened_df = unopened_df.rename(columns = {\"FID\": \"cb_station\", \"y\": \"lat\", \"x\": \"long\", \"ClosestInt\": \"name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c8d61075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "unopened_traffic_df = pd.DataFrame(columns = ['id', 'long', 'lat', 'open_year', 'name','2010', '2011', '2012', '2013', '2014', '2015'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2b67fe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_30636\\4276987942.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unopened_traffic_df = pd.concat([unopened_traffic_df, test_df], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "#script to get average traffic volume for all unopened capital bike shares\n",
    "for xy in range(len(unopened_df)):\n",
    "    #runs query for coordinates\n",
    "    test_json = traffic_query([unopened_df[\"long\"][xy], unopened_df[\"lat\"][xy]])\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test_json == \"Query Failed\" or len(test_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns = ['id', 'long', 'lat', 'open_year', 'name','2010', '2011', '2012', '2013', '2014', '2015'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['id'] = unopened_df[\"cb_station\"][xy]\n",
    "        append['long'] = unopened_df[\"long\"][xy]\n",
    "        append['lat'] = unopened_df[\"lat\"][xy]\n",
    "        append['name'] = unopened_df[\"name\"][xy]\n",
    "        #append it to traffic_df\n",
    "        unopened_traffic_df = pd.concat([unopened_traffic_df, append], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test_clean = clean_traffic(test_json, len(test_json[\"features\"]))\n",
    "    #converts to pandas and filters for where there is data for both 2013 and 2019\n",
    "    test_df = pd.DataFrame(test_clean)\n",
    "\n",
    "    #gets mean traffic volume for all years\n",
    "    test_df = test_df.agg([\"mean\"]).reset_index(drop = True)\n",
    "\n",
    "    #adds station and search coordinates\n",
    "    test_df['id'] = unopened_df[\"cb_station\"][xy]\n",
    "    test_df['long'] = unopened_df[\"long\"][xy]\n",
    "    test_df['lat'] = unopened_df[\"lat\"][xy]\n",
    "    test_df['open_year'] = np.nan\n",
    "    test_df['name'] = unopened_df[\"name\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    test_df = test_df[['id', 'long', 'lat', 'open_year', 'name','2010', '2011', '2012', '2013', '2014', '2015', '2016']]\n",
    "    #add to storage dataframe\n",
    "    unopened_traffic_df = pd.concat([unopened_traffic_df, test_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c7b16a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>170.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-77.018976</td>\n",
       "      <td>38.915297</td>\n",
       "      <td>20531.187011</td>\n",
       "      <td>20415.163689</td>\n",
       "      <td>20535.039831</td>\n",
       "      <td>20592.764541</td>\n",
       "      <td>20101.722714</td>\n",
       "      <td>19087.794349</td>\n",
       "      <td>19017.483782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.043765</td>\n",
       "      <td>0.035673</td>\n",
       "      <td>22079.293989</td>\n",
       "      <td>22209.478156</td>\n",
       "      <td>21965.549927</td>\n",
       "      <td>24932.146479</td>\n",
       "      <td>24641.086544</td>\n",
       "      <td>23694.170277</td>\n",
       "      <td>24022.633048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-77.109057</td>\n",
       "      <td>38.819474</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1417.000000</td>\n",
       "      <td>1404.000000</td>\n",
       "      <td>1410.000000</td>\n",
       "      <td>1823.000000</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1933.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-77.051052</td>\n",
       "      <td>38.893453</td>\n",
       "      <td>9364.125000</td>\n",
       "      <td>9236.937500</td>\n",
       "      <td>9132.375000</td>\n",
       "      <td>9563.375000</td>\n",
       "      <td>8899.875000</td>\n",
       "      <td>8615.333333</td>\n",
       "      <td>8920.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-77.022431</td>\n",
       "      <td>38.915921</td>\n",
       "      <td>15481.416667</td>\n",
       "      <td>15622.916667</td>\n",
       "      <td>16122.964286</td>\n",
       "      <td>15826.916667</td>\n",
       "      <td>14956.683333</td>\n",
       "      <td>14074.000000</td>\n",
       "      <td>13765.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-76.987472</td>\n",
       "      <td>38.938926</td>\n",
       "      <td>21492.906250</td>\n",
       "      <td>22581.166667</td>\n",
       "      <td>23746.375000</td>\n",
       "      <td>23023.250000</td>\n",
       "      <td>21767.937500</td>\n",
       "      <td>20161.000000</td>\n",
       "      <td>19732.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-76.919072</td>\n",
       "      <td>38.991081</td>\n",
       "      <td>175418.000000</td>\n",
       "      <td>177134.000000</td>\n",
       "      <td>175205.000000</td>\n",
       "      <td>226169.000000</td>\n",
       "      <td>226108.000000</td>\n",
       "      <td>234467.000000</td>\n",
       "      <td>241377.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             long         lat           2010           2011           2012  \\\n",
       "count  184.000000  184.000000     160.000000     160.000000     160.000000   \n",
       "mean   -77.018976   38.915297   20531.187011   20415.163689   20535.039831   \n",
       "std      0.043765    0.035673   22079.293989   22209.478156   21965.549927   \n",
       "min    -77.109057   38.819474    1500.000000    1417.000000    1404.000000   \n",
       "25%    -77.051052   38.893453    9364.125000    9236.937500    9132.375000   \n",
       "50%    -77.022431   38.915921   15481.416667   15622.916667   16122.964286   \n",
       "75%    -76.987472   38.938926   21492.906250   22581.166667   23746.375000   \n",
       "max    -76.919072   38.991081  175418.000000  177134.000000  175205.000000   \n",
       "\n",
       "                2013           2014           2015           2016  \n",
       "count     160.000000     164.000000     165.000000     170.000000  \n",
       "mean    20592.764541   20101.722714   19087.794349   19017.483782  \n",
       "std     24932.146479   24641.086544   23694.170277   24022.633048  \n",
       "min      1410.000000    1823.000000    1883.000000    1933.000000  \n",
       "25%      9563.375000    8899.875000    8615.333333    8920.833333  \n",
       "50%     15826.916667   14956.683333   14074.000000   13765.000000  \n",
       "75%     23023.250000   21767.937500   20161.000000   19732.975000  \n",
       "max    226169.000000  226108.000000  234467.000000  241377.000000  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unopened_traffic_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving traffic data for unopened capital bikes\n",
    "unopened_traffic_df.to_csv(\"unopened_cb_traffic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec22ff1",
   "metadata": {},
   "source": [
    "Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crime_query(coord_list, year):\n",
    "    \"\"\"\n",
    "    takes in a list of captial bikeshare coordinates\n",
    "    to tell API what data to retrieve\n",
    "    and the year to indicate which API to utilize\n",
    "    \"\"\"\n",
    "    \n",
    "    #extract coordinates\n",
    "    long = coord_list[0] #UPDATE LATER AFTER SEEING HOW IBADAT SET IT UP\n",
    "    lat = coord_list[1]\n",
    "    \n",
    "    #run query\n",
    "    if year == 2019:\n",
    "        B_URL = \"https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/10/query?\" #base url for 2019\n",
    "    elif year == 2013:\n",
    "        B_URL = \"https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/1/query?\" #base url for 2013\n",
    "    \n",
    "    response = requests.get(\n",
    "        B_URL,\n",
    "        params = {\n",
    "            \"where\": \"1=1\", #no filters\n",
    "            \"outFields\": \"CCN,OFFENSE,WARD,METHOD,SHIFT,LONGITUDE,LATITUDE\", #indicates which cols to return\n",
    "            \"geometry\": f\"{long},{lat}\", #input coordinates\n",
    "            \"geometryType\": \"esriGeometryPoint\", #indicates we're giving it points\n",
    "            \"distance\": 500, #how far away from point\n",
    "            \"units\": \"esriSRUnit_Meter\", #units in meters\n",
    "            \"inSR\": \"4326\", #coordiante system\n",
    "            \"f\": \"json\" #type of file to return\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #check if successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return \"Query Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_crime(geo_json, item_len, year):\n",
    "    \"\"\"\n",
    "    function to extract columns of interest from crime jsons\n",
    "    takes in the json created in crime_query() as well as the length\n",
    "    of response.json()[\"features\"]. also a int indicating year of crime api\n",
    "    \"\"\"\n",
    "    #storing\n",
    "    temp = []\n",
    "    \n",
    "    for item in range(item_len):\n",
    "        obs = {\n",
    "                \"id\": geo_json[\"features\"][item][\"attributes\"][\"CCN\"],\n",
    "                \"x\": geo_json[\"features\"][item][\"attributes\"][\"LONGITUDE\"],\n",
    "                \"y\": geo_json[\"features\"][item][\"attributes\"][\"LATITUDE\"],\n",
    "                \"ward\": geo_json[\"features\"][item][\"attributes\"][\"WARD\"],\n",
    "                \"method\": geo_json[\"features\"][item][\"attributes\"][\"METHOD\"],\n",
    "                \"shift\": geo_json[\"features\"][item][\"attributes\"][\"SHIFT\"],\n",
    "                \"offense\": geo_json[\"features\"][item][\"attributes\"][\"OFFENSE\"],\n",
    "                \"year\": year\n",
    "            }\n",
    "        temp.append(obs)    \n",
    "        \n",
    "    #return \n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "crime_df = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef6197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#script to get average crime for all opened capital bike shares\n",
    "for xy in range(len(coord_df)):\n",
    "    #runs query for coordinates\n",
    "    test13_json = crime_query([coord_df[\"long\"][xy], coord_df[\"lat\"][xy]], 2013)\n",
    "    test19_json = crime_query([coord_df[\"long\"][xy], coord_df[\"lat\"][xy]], 2019)\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test13_json == \"Query Failed\" or test19_json == \"Query Failed\" or len(test13_json[\"features\"]) == 0 or len(test19_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        #append it to crime_df\n",
    "        crime_df = pd.concat([crime_df, append], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test13_clean = clean_crime(test13_json, len(test13_json[\"features\"]), 2013)\n",
    "    test19_clean = clean_crime(test19_json, len(test19_json[\"features\"]), 2019)\n",
    "    \n",
    "    #converts to pandas\n",
    "    test13_df = pd.DataFrame(test13_clean)\n",
    "    test19_df = pd.DataFrame(test19_clean)\n",
    "    \n",
    "    #compute crime count\n",
    "    test13_df = test13_df.assign(crime_sum = len(test13_df)).filter(['crime_sum']).drop_duplicates()\n",
    "    test13_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "    test19_df = test19_df.assign(crime_sum = len(test19_df)).filter(['crime_sum']).drop_duplicates()\n",
    "    test19_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "\n",
    "    #rename cols\n",
    "    test13_df = test13_df.rename(columns={col: col + '_13' for col in test13_df.columns if col != 'station'})\n",
    "    test19_df = test19_df.rename(columns={col: col + '_19' for col in test19_df.columns if col != 'station'})\n",
    "    \n",
    "    #merge df\n",
    "    merged_df = pd.merge(test13_df, test19_df, on='station', how='outer')\n",
    "    \n",
    "    #filters for where there is data for both 2013 and 2019\n",
    "    merged_df = merged_df[(~merged_df['crime_sum_13'].isna()) & (~merged_df['crime_sum_19'].isna())]\n",
    "    \n",
    "    #adds if-else statement \n",
    "    if len(merged_df) == 0: #in case there is no row with data for both\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        #append it to crime_df\n",
    "        crime_df = pd.concat([crime_df, append], ignore_index = True)\n",
    "        continue\n",
    "    elif len(merged_df) == 1: #if there is only one row and we can't compute SEM\n",
    "        merged_df = (merged_df.assign(change_crime = merged_df[\"crime_sum_13\"] - merged_df[\"crime_sum_19\"]).\n",
    "         filter([\"change_crime\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_crime').\n",
    "         rename(columns = {\"mean\" : \"change_CRIME_mean\",\n",
    "                           \"sem\" : \"change_CRIME_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "        merged_df['change_CRIME_sem'] = np.nan\n",
    "    else:\n",
    "        #enough rows to compute both mean change in crime volume & standard error\n",
    "        merged_df = (merged_df.assign(change_crime = merged_df[\"crime_sum_13\"] - merged_df[\"crime_sum_19\"]).\n",
    "         filter([\"change_crime\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_crime').\n",
    "         rename(columns = {\"mean\" : \"change_CRIME_mean\",\n",
    "                           \"sem\" : \"change_CRIME_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "    #adds station and search coordinates\n",
    "    merged_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "    merged_df['long'] = coord_df[\"long\"][xy]\n",
    "    merged_df['lat'] = coord_df[\"lat\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    merged_df = merged_df[['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat']]\n",
    "    #add to storage dataframe\n",
    "    crime_df = pd.concat([crime_df, merged_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bd78b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saving crime data for opened capital bikes\n",
    "#crime_df.to_csv(\"opened_cb_crime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "unopened_crime_df = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15a542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#script to get average crime for all unopened capital bike shares\n",
    "for xy in range(len(unopened_df)):\n",
    "    #runs query for coordinates\n",
    "    test13_json = crime_query([unopened_df[\"long\"][xy], unopened_df[\"lat\"][xy]], 2013)\n",
    "    test19_json = crime_query([unopened_df[\"long\"][xy], unopened_df[\"lat\"][xy]], 2019)\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test13_json == \"Query Failed\" or test19_json == \"Query Failed\" or len(test13_json[\"features\"]) == 0 or len(test19_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = unopened_df[\"cb_station\"][xy]\n",
    "        append['long'] = unopened_df[\"long\"][xy]\n",
    "        append['lat'] = unopened_df[\"lat\"][xy]\n",
    "        #append it to crime_df\n",
    "        unopened_crime_df = pd.concat([unopened_crime_df, append], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test13_clean = clean_crime(test13_json, len(test13_json[\"features\"]), 2013)\n",
    "    test19_clean = clean_crime(test19_json, len(test19_json[\"features\"]), 2019)\n",
    "    \n",
    "    #converts to pandas\n",
    "    test13_df = pd.DataFrame(test13_clean)\n",
    "    test19_df = pd.DataFrame(test19_clean)\n",
    "    \n",
    "    #compute crime count\n",
    "    test13_df = test13_df.assign(crime_sum = len(test13_df)).filter(['crime_sum']).drop_duplicates()\n",
    "    test13_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "    test19_df = test19_df.assign(crime_sum = len(test19_df)).filter(['crime_sum']).drop_duplicates()\n",
    "    test19_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "\n",
    "    #rename cols\n",
    "    test13_df = test13_df.rename(columns={col: col + '_13' for col in test13_df.columns if col != 'station'})\n",
    "    test19_df = test19_df.rename(columns={col: col + '_19' for col in test19_df.columns if col != 'station'})\n",
    "    \n",
    "    #merge df\n",
    "    merged_df = pd.merge(test13_df, test19_df, on='station', how='outer')\n",
    "    \n",
    "    #filters for where there is data for both 2013 and 2019\n",
    "    merged_df = merged_df[(~merged_df['crime_sum_13'].isna()) & (~merged_df['crime_sum_19'].isna())]\n",
    "    \n",
    "    #adds if-else statement \n",
    "    if len(merged_df) == 0: #in case there is no row with data for both\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = unopened_df[\"cb_station\"][xy]\n",
    "        append['long'] = unopened_df[\"long\"][xy]\n",
    "        append['lat'] = unopened_df[\"lat\"][xy]\n",
    "        #append it to crime_df\n",
    "        unopened_crime_df = pd.concat([unopened_crime_df, append], ignore_index = True)\n",
    "        continue\n",
    "    elif len(merged_df) == 1: #if there is only one row and we can't compute SEM\n",
    "        merged_df = (merged_df.assign(change_crime = merged_df[\"crime_sum_13\"] - merged_df[\"crime_sum_19\"]).\n",
    "         filter([\"change_crime\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_crime').\n",
    "         rename(columns = {\"mean\" : \"change_CRIME_mean\",\n",
    "                           \"sem\" : \"change_CRIME_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "        merged_df['change_CRIME_sem'] = np.nan\n",
    "    else:\n",
    "        #enough rows to compute both mean change in crime volume & standard error\n",
    "        merged_df = (merged_df.assign(change_crime = merged_df[\"crime_sum_13\"] - merged_df[\"crime_sum_19\"]).\n",
    "         filter([\"change_crime\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_crime').\n",
    "         rename(columns = {\"mean\" : \"change_CRIME_mean\",\n",
    "                           \"sem\" : \"change_CRIME_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "    #adds station and search coordinates\n",
    "    merged_df['station'] = unopened_df[\"cb_station\"][xy]\n",
    "    merged_df['long'] = unopened_df[\"long\"][xy]\n",
    "    merged_df['lat'] = unopened_df[\"lat\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    merged_df = merged_df[['station', 'change_CRIME_mean', 'change_CRIME_sem', 'long', 'lat']]\n",
    "    #add to storage dataframe\n",
    "    unopened_crime_df = pd.concat([unopened_crime_df, merged_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46096057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving crime data for unopened capital bikes\n",
    "#unopened_crime_df.to_csv(\"unopened_cb_crime.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

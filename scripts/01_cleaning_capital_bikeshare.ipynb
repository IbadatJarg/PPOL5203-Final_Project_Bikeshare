{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> PPOL 5203 : Final Project  <br><br> \n",
    "<font color='grey'> Cleaning Capital Bikeshare Data <br><br>\n",
    "Ibadat Jarg and Helen Wang</center></center> <h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Librries as neened\n",
    "#pip install numpy\n",
    "#pip install pandas\n",
    "#!pip install GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing json file\n",
    "with open(r'..\\data\\raw_data\\capital_bike_Locations.geojson', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing features from data\n",
    "features = data['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "# Include 'properties' for attributes and optionally 'geometry' for coordinates\n",
    "main_df = pd.DataFrame([{\n",
    "    **feature['properties'],  # Unpack properties\n",
    "    'geometry': feature['geometry']  # Include geometry (optional)\n",
    "} for feature in features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script to read all csv files and combine them into one data frame\n",
    "\n",
    "# Directory containing CSV files - links provided in github since size of files is too large. Path provided here is local\n",
    "directory_path = r\"C:\\Users\\Ibadat\\Desktop\\PPOL5203 Data Science 1\\final_project\\bikeshare_ride_history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold dataframes\n",
    "dataframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverse the directory tree\n",
    "for root, dirs, files in os.walk(directory_path):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith(\".csv\"):  # Handle case-insensitivity for CSV files\n",
    "            file_path = os.path.join(root, filename)\n",
    "            print(f\"Reading {file_path}...\")\n",
    "            try:\n",
    "                # Read each CSV file and append to the list\n",
    "                dataframes.append(pd.read_csv(file_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dataframes into a single dataframe\n",
    "if dataframes:  # Ensure there are dataframes to concatenate\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Save combined dataframe to a new CSV (optional)\n",
    "    output_file = os.path.join(directory_path, \"combined_output.csv\")\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Combined CSV saved to {output_file}.\")\n",
    "else:\n",
    "    print(\"No CSV files found in the directory or its subfolders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting file path to read the combined data (could skip this if elected not to save output in separate file)\n",
    "file_path = r\"C:\\Users\\Ibadat\\Desktop\\PPOL5203 Data Science 1\\final_project\\bikeshare_ride_history\\combined_output.csv\"\n",
    "\n",
    "#Saving as df\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling in missing values for start date\n",
    "df['Start'] = df['Start date'].fillna(df['started_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling in missing values for start date\n",
    "df['Name'] = df['Start station'].fillna(df['start_station_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have station name and start date for all stations and can identify which year the capital bikeshare location was openned\n",
    "\n",
    "#Confirming that the format of 'Start' is correct\n",
    "df['Start'] = pd.to_datetime(df['Start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping by the lowest values in the data column\n",
    "earliest_dates = df.groupby('Name')['Start'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for opening years\n",
    "opening_years = earliest_dates.dt.year.reset_index()\n",
    "opening_years.rename(columns={'Start': 'Opening Year'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the opening year back into the original DataFrame\n",
    "df = df.merge(opening_years, on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'df' has unique 'Name' and 'Opening Year'\n",
    "df_unique = df.drop_duplicates(subset=['Name'])[['Name', 'Opening Year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge main_df with the unique 'Opening Year' from df, specifying the column names\n",
    "main_df = main_df.merge(df_unique, left_on='NAME', right_on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated main_df\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving file\n",
    "main_df.to_csv(\"../data/cleaned_data/opened_capital_bikes.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

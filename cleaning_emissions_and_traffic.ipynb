{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7dbe1c6",
   "metadata": {},
   "source": [
    "<h1><center> Data Cleaning File  <br><br> \n",
    "<font color='grey'> Cleaning Emissions and Traffic Data <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81cb5b1",
   "metadata": {},
   "source": [
    "Emissions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82614a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in emissions data files\n",
    "emissions_16 = pd.read_excel(\"2016_emissions.xlsx\",\n",
    "                            sheet_name = \"Direct Emitters\", #only picks first sheet\n",
    "                            header = 3,\n",
    "                            usecols = [\"City\", \"State\", \"Zip Code\", \"County\", \"Latitude\", \"Longitude\", \"Industry Type (subparts)\", \"Industry Type (sectors)\", \"Total reported direct emissions\"])\n",
    "emissions_13 = pd.read_excel(\"2013_emissions.xlsx\",\n",
    "                            sheet_name = \"Direct Emitters\", #only picks first sheet\n",
    "                            header = 3,\n",
    "                            usecols = [\"City\", \"State\", \"Zip Code\", \"County\", \"Latitude\", \"Longitude\", \"Industry Type (subparts)\", \"Industry Type (sectors)\", \"Total reported direct emissions\"])\n",
    "emissions_19 = pd.read_excel(\"2019_emissions.xlsx\",\n",
    "                            sheet_name = \"Direct Emitters\", #only picks first sheet\n",
    "                            header = 3,\n",
    "                            usecols = [\"City\", \"State\", \"Zip Code\", \"County\", \"Latitude\", \"Longitude\", \"Industry Type (subparts)\", \"Industry Type (sectors)\", \"Total reported direct emissions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92225f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column names\n",
    "new_col_names = [\"city\", \"state\", \"zipcode\", \"county\", \"latitude\", \"longitude\", \"industry_sub\", \"industry_main\", \"total_emissions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbcb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "for df in [emissions_16, emissions_13, emissions_19]:\n",
    "    df.columns = new_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a90ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save list of relevant states and industry\n",
    "rel_states = [\"DC\", \"MD\", \"VA\"]\n",
    "rel_indust = \"MN|NN|C|W\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for dc, md, and va + relevant industries\n",
    "emissions_16 = emissions_16.query('state in @rel_states & industry_sub.str.contains(@rel_indust, na = False)')\n",
    "emissions_13 = emissions_13.query('state in @rel_states & industry_sub.str.contains(@rel_indust, na = False)')\n",
    "emissions_19 = emissions_19.query('state in @rel_states & industry_sub.str.contains(@rel_indust, na = False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17dfa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking number of rows\n",
    "[df.shape for df in [emissions_16, emissions_13, emissions_19]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding identifers before merging\n",
    "emissions_13.loc[:, \"time\"] = 2013\n",
    "emissions_16.loc[:, \"time\"] = 2016\n",
    "emissions_19.loc[:, \"time\"] = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dac4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "emissions = emissions_13.merge(emissions_16, how = \"outer\").merge(emissions_19, how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f98184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to wide format\n",
    "emissions = emissions.pivot_table(\n",
    "    index = list(emissions.columns[:-2]),\n",
    "    columns = 'time',\n",
    "    values = 'total_emissions'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set all colnames to str\n",
    "emissions.columns = emissions.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765786ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filters for all data that has both 2013 and 2019\n",
    "emissions = emissions[(~emissions['2013'].isna()) & (~emissions['2019'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502fb91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emissions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd7b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emissions.query(\"state == 'DC'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff75c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emissions.query(\"state == 'MD'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions.query(\"state == 'VA'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9ffd5",
   "metadata": {},
   "source": [
    "Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7daf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting library\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa853685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traffic_query(coord_list):\n",
    "    \"\"\"\n",
    "    takes in a list of captial bikeshare coordinates\n",
    "    to tell API what data to retrieve\n",
    "    \"\"\"\n",
    "    \n",
    "    #extract coordinates\n",
    "    long = coord_list[0] #UPDATE LATER AFTER SEEING HOW IBADAT SET IT UP\n",
    "    lat = coord_list[1]\n",
    "    \n",
    "    #run query\n",
    "    B_URL = \"https://gis.mwcog.org/wa/rest/services/RTDC/Traffic_Counts_Annual/MapServer/0/query?\" #base url\n",
    "    \n",
    "    response = requests.get(\n",
    "        B_URL,\n",
    "        params = {\n",
    "            \"where\": \"1=1\", #no filters\n",
    "            \"outFields\": \"STATION,COUNTY,AADT2013,AADT2016, AADT2019,XCOORD,YCOORD\", #indicates which cols to return\n",
    "            \"geometry\": f\"{long},{lat}\", #input coordinates\n",
    "            \"geometryType\": \"esriGeometryPoint\", #indicates we're giving it points\n",
    "            \"distance\": 500, #how far away from point\n",
    "            \"units\": \"esriSRUnit_Meter\", #units in meters\n",
    "            \"inSR\": \"4326\", #coordiante system\n",
    "            \"f\": \"json\" #type of file to return\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #check if successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return \"Query Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb1a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_traffic(geo_json, item_len):\n",
    "    \"\"\"\n",
    "    function to extract columns of interest from traffic jsons\n",
    "    takes in the json created in traffic_query() as well as the length\n",
    "    of response.json()[\"features\"]\n",
    "    \"\"\"\n",
    "    #storing\n",
    "    temp = []\n",
    "    \n",
    "    for item in range(item_len):\n",
    "        obs = {\n",
    "                \"id\": geo_json[\"features\"][item][\"attributes\"][\"STATION\"],\n",
    "                \"x\": geo_json[\"features\"][item][\"attributes\"][\"XCOORD\"],\n",
    "                \"y\": geo_json[\"features\"][item][\"attributes\"][\"YCOORD\"],\n",
    "                \"county\": geo_json[\"features\"][item][\"attributes\"][\"COUNTY\"],\n",
    "                \"2013\": geo_json[\"features\"][item][\"attributes\"][\"AADT2013\"],\n",
    "                \"2016\": geo_json[\"features\"][item][\"attributes\"][\"AADT2016\"],\n",
    "                \"2019\": geo_json[\"features\"][item][\"attributes\"][\"AADT2019\"]\n",
    "            }\n",
    "        temp.append(obs)    \n",
    "        \n",
    "    #return \n",
    "    return(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e893bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_21996\\1224742629.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  coord_df = pd.concat([coord_df, coordinates], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "#test coordinates \n",
    "coord_df = pd.DataFrame(columns=['cb_station', 'long', 'lat'])\n",
    "\n",
    "coordinates = pd.DataFrame([\n",
    "    {'cb_station': \"a\", 'long': -77.0334, 'lat': 38.89223},\n",
    "    {'cb_station': \"b\", 'long': -77.1334, 'lat': 38.87223}\n",
    "])\n",
    "\n",
    "coord_df = pd.concat([coord_df, coordinates], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "813a5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for storage\n",
    "traffic_df = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dacb2a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_21996\\25788858.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "for xy in range(len(coord_df)):\n",
    "    #runs query for coordinates\n",
    "    test_json = traffic_query([coord_df[\"long\"][xy], coord_df[\"lat\"][xy]])\n",
    "    \n",
    "    #adds if statement in case query fails or returns no coordinates\n",
    "    if test_json == \"Query Failed\" or len(test_json[\"features\"]) == 0:\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        #append it to traffic_df\n",
    "        traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n",
    "        continue\n",
    "        \n",
    "    #cleans up resulting json\n",
    "    test_clean = clean_traffic(test_json, len(test_json[\"features\"]))\n",
    "    #converts to pandas and filters for where there is data for both 2013 and 2019\n",
    "    test_df = pd.DataFrame(test_clean)\n",
    "    test_df = test_df[(~test_df['2013'].isna()) & (~test_df['2019'].isna())]\n",
    "    \n",
    "    #adds if-else statement \n",
    "    if len(test_df) == 0: #in case there is no row with data for both\n",
    "        #creates dataframe with NaN values for AADT\n",
    "        append = pd.DataFrame(columns=['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat'])\n",
    "        append.loc[len(append)] = np.nan\n",
    "        append['station'] = coord_df[\"cb_station\"][xy]\n",
    "        append['long'] = coord_df[\"long\"][xy]\n",
    "        append['lat'] = coord_df[\"lat\"][xy]\n",
    "        #append it to traffic_df\n",
    "        traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n",
    "        continue\n",
    "    elif len(test_df) == 1: #if there is only one row and we can't compute SEM\n",
    "        test_df = (test_df.assign(change_AADT = test_df[\"2013\"] - test_df[\"2019\"]).\n",
    "         filter([\"change_AADT\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_AADT').\n",
    "         rename(columns = {\"mean\" : \"change_AADT_mean\",\n",
    "                           \"sem\" : \"change_AADT_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "        test_df['change_AADT_sem'] = np.nan\n",
    "    else:\n",
    "        #enough rows to compute both mean change in traffic volume & standard error\n",
    "        test_df = (test_df.assign(change_AADT = test_df[\"2013\"] - test_df[\"2019\"]).\n",
    "         filter([\"change_AADT\"]).\n",
    "         agg([\"mean\", \"sem\"]).\n",
    "         reset_index().\n",
    "         pivot_table(\n",
    "                index = None,\n",
    "                columns = 'index',\n",
    "                values = 'change_AADT').\n",
    "         rename(columns = {\"mean\" : \"change_AADT_mean\",\n",
    "                           \"sem\" : \"change_AADT_sem\"}).\n",
    "         reset_index(drop=True)\n",
    "         )\n",
    "    #adds station and search coordinates\n",
    "    test_df['station'] = coord_df[\"cb_station\"][xy]\n",
    "    test_df['long'] = coord_df[\"long\"][xy]\n",
    "    test_df['lat'] = coord_df[\"lat\"][xy]\n",
    "    #reorder cols to match storage dataframe\n",
    "    test_df = test_df[['station', 'change_AADT_mean', 'change_AADT_sem', 'long', 'lat']]\n",
    "    #add to storage dataframe\n",
    "    traffic_df = pd.concat([traffic_df, test_df], ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd7415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>change_AADT_mean</th>\n",
       "      <th>change_AADT_sem</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>-1825.4</td>\n",
       "      <td>1966.061756</td>\n",
       "      <td>-77.0334</td>\n",
       "      <td>38.89223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.1334</td>\n",
       "      <td>38.87223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station  change_AADT_mean  change_AADT_sem     long       lat\n",
       "0       a           -1825.4      1966.061756 -77.0334  38.89223\n",
       "1       b            2000.0              NaN -77.1334  38.87223"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
